{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fcaf9b",
   "metadata": {},
   "source": [
    "# 517 M1 Assignment - TicTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3a6de",
   "metadata": {},
   "source": [
    "## Environment:\n",
    "### When it initializes, it creates an np array with 0's in each of the three cells. Xes are -1 and Oes are 1. We are going to assume the agent gets to go first. The \"AI\" of the opponent is any valid move selected uniform random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68950f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: X, -1: O\n",
    "        self.current_player = 1  # Player 1 (X) starts\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.board[action] != 0:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        self.board[action] = self.current_player\n",
    "        reward, done = self.check_game_status()\n",
    "\n",
    "        self.current_player = -self.current_player#Returns a 1 for win, .5 for draw, 0 for loss\n",
    "        return self.board, reward, done\n",
    "\n",
    "    def check_game_status(self):\n",
    "        for player in [1, -1]:\n",
    "            # Check rows and columns\n",
    "            for i in range(3):\n",
    "                if all(self.board[i, :] == player) or all(self.board[:, i] == player):\n",
    "                    return player, True\n",
    "            # Check diagonals\n",
    "            if self.board[0, 0] == self.board[1, 1] == self.board[2, 2] == player:\n",
    "                return player, True\n",
    "            if self.board[0, 2] == self.board[1, 1] == self.board[2, 0] == player:\n",
    "                return player, True\n",
    "\n",
    "        # Check for draw\n",
    "        if not self.available_actions():\n",
    "            return .5, True\n",
    "\n",
    "        # Game is still ongoing\n",
    "        return 0, False\n",
    "    \n",
    "    def get_short_string_state(self):\n",
    "        return str(test_game_env.board.tolist())\n",
    "    \n",
    "    #Useful for looking ahead when making a greedy algorithm\n",
    "    def get_short_string_state_look_ahead(self, action):\n",
    "        if self.board[action] != 0:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        self.board[action] = self.current_player\n",
    "        self.board[action] = 0# make empty again\n",
    "        return str(test_game_env.board.tolist())\n",
    "        \n",
    "    def render(self):\n",
    "        symbol_map = {1: 'X', -1: 'O', 0: ' '}\n",
    "        for row in self.board:\n",
    "            print(' | '.join(symbol_map[cell] for cell in row))\n",
    "            print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23688c",
   "metadata": {},
   "source": [
    "### Let's initialize the game and play a short game, were X moves across the top row and O moves across the bottom. O has a poor strategy because it goes second and will lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "603cba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank Game\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "game status (0, False) \n",
      "\n",
      "Actions available: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "X moves to the top left\n",
      "X |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "game status (0, False) \n",
      "\n",
      "Actions available: [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "O moves to the bottom right\n",
      "X |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "game status (0, False) \n",
      "\n",
      "Actions available: [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n",
      "X moves to the top middle\n",
      "X | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   | O\n",
      "---------\n",
      "game status (0, False) \n",
      "\n",
      "Actions available: [(0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n",
      "O moves to the bottom middle\n",
      "X | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "game status (0, False) \n",
      "\n",
      "Actions available: [(0, 2), (1, 0), (1, 1), (1, 2), (2, 0)]\n",
      "X moves to the top right (and wins)\n",
      "X | X | X\n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  | O | O\n",
      "---------\n",
      "game status (1, True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_game_env = TicTacToe()\n",
    "print(\"Blank Game\")\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n",
    "print(\"Actions available:\",test_game_env.available_actions())\n",
    "print(\"X moves to the top left\")\n",
    "test_game_env.step(test_game_env.available_actions()[0])\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n",
    "print(\"Actions available:\",test_game_env.available_actions())\n",
    "print(\"O moves to the bottom right\")\n",
    "test_game_env.step(test_game_env.available_actions()[7])\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n",
    "print(\"Actions available:\",test_game_env.available_actions())\n",
    "print(\"X moves to the top middle\")\n",
    "test_game_env.step(test_game_env.available_actions()[0])\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n",
    "print(\"Actions available:\",test_game_env.available_actions())\n",
    "print(\"O moves to the bottom middle\")\n",
    "test_game_env.step(test_game_env.available_actions()[5])\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n",
    "print(\"Actions available:\",test_game_env.available_actions())\n",
    "print(\"X moves to the top right (and wins)\")\n",
    "test_game_env.step(test_game_env.available_actions()[0])\n",
    "test_game_env.render()\n",
    "print(\"game status\", test_game_env.check_game_status(),\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5b16b",
   "metadata": {},
   "source": [
    "### Two agents: One that simply picks randomly (RandAgent) and another that tries to go in the same pattern every time (SimpleAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class RandAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state, available_actions):\n",
    "        return random.choice(available_actions)\n",
    "    \n",
    "    ## for a smarter agent, this would inform the agent of a game position and whether the game was lost or won\n",
    "    def update(self, state, reward, gameover=True):\n",
    "        pass\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self):\n",
    "        self.favorite_moves = [(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)]\n",
    "    \n",
    "    def policy(self, state, available_actions):\n",
    "        for i in self.favorite_moves:\n",
    "            if i in available_actions:\n",
    "                return i\n",
    "        return random.choice(available_actions)#this will never happen\n",
    "    \n",
    "    ## for a smarter agent, this would inform the agent of a game position and whether the game was lost or won\n",
    "    def update(self, state, reward, gameover=True):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "#This function returns a summary of games won by agent1 (X) against agent2 (O).\n",
    "#Remember that a loss or draw for O is kind of a win, since X has a significant advantage.\n",
    "def evaluate_agents(agent1,agent2, num_games=1000):\n",
    "    env = TicTacToe()\n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    loss_count = 0\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        use_agent1 = True\n",
    "        while not done:\n",
    "            available_actions = env.available_actions()\n",
    "            action = None\n",
    "            if use_agent1:\n",
    "                action = agent1.policy(state, available_actions)\n",
    "            else:\n",
    "                action = agent2.policy(state, available_actions)\n",
    "            state, reward, done = env.step(action)\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    win_count += 1\n",
    "                elif reward == -1:\n",
    "                    loss_count += 1\n",
    "                else:\n",
    "                    draw_count += 1\n",
    "            if use_agent1 == True:\n",
    "                use_agent1 = False\n",
    "            else:\n",
    "                use_agent1 = True\n",
    "\n",
    "    return win_count, draw_count, loss_count\n",
    "\n",
    "rand_agent = RandAgent()\n",
    "simple_agent = SimpleAgent()\n",
    "win_count, draw_count, loss_count = evaluate_agents(simple_agent,rand_agent,100)\n",
    "print(f'Wins: {win_count}, Draws: {draw_count}, Losses: {loss_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ed745",
   "metadata": {},
   "source": [
    "### GreedyAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "97d6fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "class MyGreedyAgent:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.1, epsilon_min=0.01, learning=True):\n",
    "        self.values = {}  # State-value function\n",
    "        self.game_history_strings = []  # Stores boards \n",
    "        self.learning = learning  # Learning phase \n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        # Populate values with all valid boards, marking X wins as 1, draws/other as .5, and losses as 0.\n",
    "        self.populate_values()\n",
    "\n",
    "    def check_winner(self, board, player):\n",
    "        # Checks for a win\n",
    "        win_conditions = [\n",
    "            np.all(board[i, :] == player) for i in range(3)  # Rows\n",
    "        ] + [\n",
    "            np.all(board[:, j] == player) for j in range(3)  # Columns\n",
    "        ] + [\n",
    "            np.all(np.diag(board) == player),  # Main diagonal\n",
    "            np.all(np.diag(np.fliplr(board)) == player)  # Anti-diagonal\n",
    "        ]\n",
    "        return any(win_conditions)\n",
    "\n",
    "    def all_positions(self):\n",
    "        # All possible combinations of board positions\n",
    "        all_positions = itertools.product([0, 1, -1], repeat=9)\n",
    "        return all_positions\n",
    "\n",
    "    def populate_values(self):\n",
    "        # Populates value function with all valid boards\n",
    "        for position in self.all_positions():\n",
    "            board = np.array(position).reshape(3, 3)\n",
    "            board_str = self.get_board_string(board)\n",
    "            if self.check_winner(board, 1):  # X (1) wins\n",
    "                self.values[board_str] = 1.0\n",
    "            elif self.check_winner(board, -1):  # O (-1) wins\n",
    "                self.values[board_str] = 0.0\n",
    "            else:  # Draw\n",
    "                self.values[board_str] = 0.5\n",
    "\n",
    "    def get_board_string(self, board):\n",
    "        # Convert board to a string\n",
    "        return str(board.reshape(9))\n",
    "\n",
    "    # Exploratory/best estimate to move X\n",
    "    def policy(self, state, available_actions):\n",
    "        state_str = self.get_board_string(state)\n",
    "        r = random.random()\n",
    "        if r < self.epsilon and self.learning is True:\n",
    "            # Exploratory action\n",
    "            move = random.choice(available_actions)\n",
    "        else:\n",
    "            # Greedy/best action\n",
    "            move = self.greedy(state, available_actions)\n",
    "        self.game_history_strings.append((state_str, move))  # Track history\n",
    "        return move\n",
    "\n",
    "    # This method should use self.values to select an optimal move.\n",
    "    def greedy(self, state, available_actions):\n",
    "        best_value = -float('inf')\n",
    "        best_action = None\n",
    "        state_str = self.get_board_string(state)\n",
    "        for action in available_actions:\n",
    "            next_state = state.copy()\n",
    "            next_state[action] = 1  # Assume X plays\n",
    "            next_state_str = self.get_board_string(next_state)\n",
    "            if next_state_str in self.values and self.values[next_state_str] > best_value:\n",
    "                best_value = self.values[next_state_str]\n",
    "                best_action = action\n",
    "        if best_action is None:\n",
    "            # If no best action is found, return a random move\n",
    "            return random.choice(available_actions)\n",
    "        return best_action\n",
    "\n",
    "    # This method is called in our learning loop each time a game ends.\n",
    "    def update(self, state, reward, gameover=True):\n",
    "        if self.learning:\n",
    "            for state_str, _ in reversed(self.game_history_strings):\n",
    "                old_value = self.values[state_str]\n",
    "                self.values[state_str] += self.alpha * (reward - old_value)  # TD(0) update\n",
    "                reward = self.values[state_str]  # Propagate reward backwards\n",
    "        self.game_history_strings = []  # Clears history after the update\n",
    "\n",
    "    def decay_hyperparameters(self):\n",
    "        # Decay exploration rate (epsilon) after each game\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e860cf4",
   "metadata": {},
   "source": [
    "### Training loop: This training loop trains MyGreedyAgent against the SimpleAgent/RandAgent seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3cecf0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleAgent training: - Wins: 1000, Draws: 0, Losses: 0\n",
      "RandAgent training: - Wins: 811, Draws: 43, Losses: 146\n"
     ]
    }
   ],
   "source": [
    "def train_td_lookahead_agent(agent, opponent, num_games=1000, explore=True):\n",
    "    env = TicTacToe()  # Tic-Tac-Toe environment (above)\n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    for game in range(num_games):\n",
    "        state = env.reset()  # Reset the board for a new game\n",
    "        done = False\n",
    "        use_greedy_agent = True  # Alternate turns\n",
    "\n",
    "        while not done:\n",
    "            available_actions = env.available_actions()  # Get available moves\n",
    "\n",
    "            if use_greedy_agent:\n",
    "                action = agent.policy(state, available_actions, explore=explore)  # Agent's move\n",
    "            else:\n",
    "                action = opponent.policy(state, available_actions)  # Opponent's move\n",
    "\n",
    "            next_state, reward, done = env.step(action)  # Apply move, get reward, check if game is done\n",
    "\n",
    "            if done:\n",
    "                if reward == 1:  # Agent wins\n",
    "                    win_count += 1\n",
    "                elif reward == 0.5:  # Draw/Tie\n",
    "                    draw_count += 1\n",
    "                else:  # Opponent wins\n",
    "                    loss_count += 1\n",
    "                \n",
    "                agent.update_value_function(state, action, reward, next_state)  # Update agent's strategy\n",
    "\n",
    "            use_greedy_agent = not use_greedy_agent  # Switch turns\n",
    "\n",
    "        agent.decay_hyperparameters()  # Adjust learning rate and exploration rate\n",
    "    \n",
    "    return win_count, draw_count, loss_count  # Return results of the training\n",
    "\n",
    "# Function to train the agent against SimpleAgent/RandAgent\n",
    "def train_agent(td_agent, rand_agent, simple_agent, num_games=1000):\n",
    "    \n",
    "    # Train against SimpleAgent (no exploration, as it's predictable)\n",
    "    td_agent.epsilon = 0.0\n",
    "    td_agent.alpha = 0.5\n",
    "    wins_simple, draws_simple, losses_simple = train_td_lookahead_agent(td_agent, simple_agent, num_games, explore=False)\n",
    "    \n",
    "    # Train against RandAgent (with exploration to handle randomness)\n",
    "    td_agent.epsilon = 0.4\n",
    "    td_agent.alpha = 0.3\n",
    "    wins_rand, draws_rand, losses_rand = train_td_lookahead_agent(td_agent, rand_agent, num_games, explore=True)\n",
    "    \n",
    "    return (wins_simple, draws_simple, losses_simple), (wins_rand, draws_rand, losses_rand)\n",
    "\n",
    "# Initialize the agent and opponents\n",
    "td_lookahead_agent = TDLookaheadAgentSB(alpha=0.5, gamma=0.9, epsilon=0.1)\n",
    "rand_agent = RandAgent()\n",
    "simple_agent = SimpleAgent()\n",
    "\n",
    "# Train the agent against SimpleAgent/RandAgent\n",
    "results_simple, results_rand = train_agent(td_lookahead_agent, rand_agent, simple_agent, num_games=1000)\n",
    "\n",
    "# Output\n",
    "print(f\"SimpleAgent training: - Wins: {results_simple[0]}, Draws: {results_simple[1]}, Losses: {results_simple[2]}\")\n",
    "print(f\"RandAgent training: - Wins: {results_rand[0]}, Draws: {results_rand[1]}, Losses: {results_rand[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ef1c3",
   "metadata": {},
   "source": [
    "### Evaluate: This section evaluates the agent against the simple/rand agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "08c66ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance against chosen agent - Wins: 880, Draws: 9, Losses: 111\n"
     ]
    }
   ],
   "source": [
    "greedy_agent.learning = False\n",
    "\n",
    "# Switch the comment depending on what GreedyAgent is being evaluated against\n",
    "\n",
    "# win_count, draw_count, loss_count = evaluate_agents(greedy_agent,simple_agent)\n",
    "win_count, draw_count, loss_count = evaluate_agents(greedy_agent,rand_agent)\n",
    "\n",
    "print(f'Performance against chosen agent - Wins: {win_count}, Draws: {draw_count}, Losses: {loss_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6e4f1",
   "metadata": {},
   "source": [
    "## Extra Greedy Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060a912",
   "metadata": {},
   "source": [
    "### Greedy Agent - Extra 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7898cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# MyGreedyAgent Class Definition (Aligned with SB)\n",
    "class MyGreedyAgent:\n",
    "    def __init__(self, alpha=.01, epsilon=.1, gamma=0.9, learning=True):\n",
    "        self.values = {}  # State-value function\n",
    "        self.game_history_strings = []  # Stores boards to trace back later\n",
    "        self.learning = learning  # Learning phase or greedy phase\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        \n",
    "        # Populate the value function with all valid boards\n",
    "        self.populate_values()\n",
    "\n",
    "    def populate_values(self):\n",
    "        # Generate all possible board positions and assign initial values\n",
    "        for position in self.all_positions():\n",
    "            board = np.array(position).reshape(3, 3)\n",
    "            board_str = self.get_board_string(board)\n",
    "            if self.check_winner(board, 1):  # X (1) wins\n",
    "                self.values[board_str] = 1.0\n",
    "            elif self.check_winner(board, -1):  # O (-1) wins\n",
    "                self.values[board_str] = 0.0\n",
    "            else:  # Draw or non-terminal state\n",
    "                self.values[board_str] = 0.5\n",
    "\n",
    "    def all_positions(self):\n",
    "        # Generate all possible combinations of board positions\n",
    "        all_positions = itertools.product([0, 1, -1], repeat=9)\n",
    "        return all_positions\n",
    "\n",
    "    def get_board_string(self, board):\n",
    "        # Convert board to a string format to store in the value function\n",
    "        return str(board.reshape(9))\n",
    "\n",
    "    def check_winner(self, board, player):\n",
    "        # Check rows, columns, and diagonals for a win\n",
    "        win_conditions = [\n",
    "            np.all(board[i, :] == player) for i in range(3)  # Rows\n",
    "        ] + [\n",
    "            np.all(board[:, j] == player) for j in range(3)  # Columns\n",
    "        ] + [\n",
    "            np.all(np.diag(board) == player),  # Main diagonal\n",
    "            np.all(np.diag(np.fliplr(board)) == player)  # Anti-diagonal\n",
    "        ]\n",
    "        return any(win_conditions)\n",
    "\n",
    "    # This method uses either exploratory (random) or greedy (optimal estimate) to move X.\n",
    "    def policy(self, state, available_actions):\n",
    "        state_str = self.get_board_string(state)\n",
    "        if random.random() < self.epsilon and self.learning:\n",
    "            # Exploratory action\n",
    "            move = random.choice(available_actions)\n",
    "        else:\n",
    "            # Greedy action\n",
    "            move = self.greedy(state, available_actions)\n",
    "        self.game_history_strings.append((state_str, move))  # Track history\n",
    "        return move\n",
    "\n",
    "    # This method selects the optimal move using self.values\n",
    "    def greedy(self, state, available_actions):\n",
    "        best_value = -float('inf')\n",
    "        best_action = None\n",
    "        for action in available_actions:\n",
    "            next_state = state.copy()\n",
    "            next_state[action] = 1  # Assume X makes the move\n",
    "            next_state_str = self.get_board_string(next_state)\n",
    "            if next_state_str in self.values and self.values[next_state_str] > best_value:\n",
    "                best_value = self.values[next_state_str]\n",
    "                best_action = action\n",
    "        if best_action is None:\n",
    "            # If no best action is found, return a random move\n",
    "            return random.choice(available_actions)\n",
    "        return best_action\n",
    "\n",
    "    # This method is called in our learning loop each time a game ends.\n",
    "    def update(self, state, reward, gameover=True):\n",
    "        if self.learning:\n",
    "            # Update value function for each state-action pair in the game history\n",
    "            for state_str, _ in reversed(self.game_history_strings):\n",
    "                old_value = self.values[state_str]\n",
    "                self.values[state_str] += self.alpha * (reward - old_value)  # TD(0) update rule\n",
    "                reward = self.values[state_str]  # Propagate the reward backwards\n",
    "        self.game_history_strings = []  # Clear history after the update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cdb46",
   "metadata": {},
   "source": [
    "### Greedy Agent - Extra 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a1473ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# TDLookaheadAgent Class Definition (Bare-Bones)\n",
    "class TDLookaheadAgent:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.4, epsilon_min=0.01, alpha_decay=0.999):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        self.alpha_decay = alpha_decay  # Learning rate decay\n",
    "        self.value_function = {}  # State-value function initialization\n",
    "    \n",
    "    def get_state(self, board):\n",
    "        \"\"\"Convert board state to a hashable format (tuple of tuples).\"\"\"\n",
    "        return tuple(map(tuple, board))\n",
    "    \n",
    "    def initialize_state_value(self, state):\n",
    "        \"\"\"Initialize state value if not already present in the value function.\"\"\"\n",
    "        if state not in self.value_function:\n",
    "            self.value_function[state] = 1.0  # Optimistic initialization\n",
    "    \n",
    "    def policy(self, state, available_actions, explore=True):\n",
    "        \"\"\"Epsilon-greedy action selection based on state values.\"\"\"\n",
    "        state_key = self.get_state(state)\n",
    "        self.initialize_state_value(state_key)\n",
    "        \n",
    "        # Epsilon-greedy: Explore with probability epsilon\n",
    "        if explore and random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        \n",
    "        # Greedy action selection based on state values\n",
    "        best_value = -float('inf')\n",
    "        best_action = None\n",
    "        for action in available_actions:\n",
    "            next_state = state.copy()\n",
    "            next_state[action] = 1  # Assume the agent plays as '1' (X)\n",
    "            next_state_key = self.get_state(next_state)\n",
    "            self.initialize_state_value(next_state_key)\n",
    "            \n",
    "            if self.value_function[next_state_key] > best_value:\n",
    "                best_value = self.value_function[next_state_key]\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def update_value_function(self, board, action, reward, next_board):\n",
    "        \"\"\"Update state value function using the TD(0) learning rule.\"\"\"\n",
    "        state = self.get_state(board)\n",
    "        next_state = self.get_state(next_board)\n",
    "        \n",
    "        self.initialize_state_value(state)\n",
    "        self.initialize_state_value(next_state)\n",
    "        \n",
    "        # TD(0) Update Rule: V(s) = V(s) + α * (reward + γ * V(s') - V(s))\n",
    "        self.value_function[state] += self.alpha * (\n",
    "            reward + self.gamma * self.value_function[next_state] - self.value_function[state]\n",
    "        )\n",
    "    \n",
    "    def decay_hyperparameters(self):\n",
    "        \"\"\"Decay exploration rate (epsilon) and learning rate (alpha) after each game.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= 0.99  # Decay epsilon by 1% each game\n",
    "        self.alpha *= self.alpha_decay  # Decay learning rate slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054fb3d",
   "metadata": {},
   "source": [
    "# Summary and Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193d4e8",
   "metadata": {},
   "source": [
    "## Q1: Summarize the Performance of your Agent(s) Across both the RandomAgent and SimpleAgent.\n",
    "\n",
    "### Answer: \n",
    "\n",
    "- The greedy agent had a strong performance against the SimpleAgent, achieving a 100% win rate with no ties or losses. This is because the SimpleAgent plays the same moves every time, allowing the greedy agent to easily predict and counter its actions without needing exploration.\n",
    "\n",
    "- The greedy agent performed well against the RandAgent, achieving around an 80% win rate. The presence of losses and draws is due to the RandAgent's random move selection, making its behavior less predictable and more challenging for the greedy agent to consistently counter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b8f9b",
   "metadata": {},
   "source": [
    "## Q2: How many games did your GreedyAgent have to train before it could beat the SimpleAgent?\n",
    "\n",
    "### Answer: \n",
    "\n",
    "- Fewer than 5 games were required. I reduced the number of games played from 1000 to 50, then to 10, 5, and finally 1. In each case, the agent achieved a 100% win rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341ff24",
   "metadata": {},
   "source": [
    "## Q3: Was RandomAgent or SimpleAgent more difficult to train an agent for? Why?\n",
    "### Answer: \n",
    "\n",
    "- Training the agent against the RandomAgent was more challenging than against the SimpleAgent. The SimpleAgent follows a predictable pattern, making it easy to counter once learned. The RandomAgent's unpredictable moves required the agent to explore and adapt to different situations, making training more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cc06f",
   "metadata": {},
   "source": [
    "## Q4: What other kinds of games do you think you could similarly train an agent for?\n",
    "### Answer: \n",
    "\n",
    "- I think this could be used for a lot of board games. The first that came to mind for me were chess and checkers. \n",
    "- On the other hand, altough it would be a lot more complex, I think it would be good for arcade games like tetris or pac man."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
